{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"19I3unK3OIxSbiXQdNicbmgtyH4AkXhrX","authorship_tag":"ABX9TyO1X/tiwLkWKx4Y7iJLPdFi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xNjyRT8DoMtD","executionInfo":{"status":"ok","timestamp":1764473288944,"user_tz":-540,"elapsed":410719,"user":{"displayName":"박준영","userId":"08318675183777163575"}},"outputId":"d7bd949d-21ac-4733-c07b-cba727d5963c"},"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","Pure Switching Latent Dynamical Systems (SLDS)\n","================================================================================\n","\n","Loading UCI-HAR dataset...\n","train set: 7352 samples, 128 timesteps, 9 channels\n","test set: 2947 samples, 128 timesteps, 9 channels\n","\n","Initializing model...\n","Total parameters: 261,638\n","\n","Starting training...\n","[Epoch 10] warmup=0.11, λ_cons=0.225, λ_div=0.090, λ_align=0.002\n","Epoch 10/150 (2.3s) | Train Loss: 0.1236, Train Acc: 0.9467 | Test Acc: 0.9277, Test F1: 0.9287 | Best F1: 0.9318\n","  Mode Usage: M0:33.9% ▇▇▇ | M1: 7.3%  | M2:18.3% ▇ | M3:11.3% ▇ | M4:12.5% ▇ | M5:16.6% ▇\n","[Epoch 20] warmup=0.24, λ_cons=0.475, λ_div=0.190, λ_align=0.005\n","Epoch 20/150 (2.4s) | Train Loss: 0.1047, Train Acc: 0.9573 | Test Acc: 0.9087, Test F1: 0.9058 | Best F1: 0.9459\n","  Mode Usage: M0:29.6% ▇▇ | M1:14.4% ▇ | M2:18.2% ▇ | M3: 8.7%  | M4:11.9% ▇ | M5:17.2% ▇\n","[Epoch 30] warmup=0.36, λ_cons=0.725, λ_div=0.290, λ_align=0.007\n","Epoch 30/150 (2.4s) | Train Loss: 0.1110, Train Acc: 0.9572 | Test Acc: 0.9304, Test F1: 0.9325 | Best F1: 0.9469\n","  Mode Usage: M0:24.9% ▇▇ | M1:10.5% ▇ | M2:17.3% ▇ | M3:12.1% ▇ | M4:15.6% ▇ | M5:19.6% ▇\n","[Epoch 40] warmup=0.49, λ_cons=0.975, λ_div=0.390, λ_align=0.010\n","Epoch 40/150 (3.5s) | Train Loss: 0.0995, Train Acc: 0.9601 | Test Acc: 0.9423, Test F1: 0.9431 | Best F1: 0.9469\n","  Mode Usage: M0:25.1% ▇▇ | M1:13.7% ▇ | M2:18.5% ▇ | M3:10.9% ▇ | M4:16.2% ▇ | M5:15.7% ▇\n","[Epoch 50] warmup=0.61, λ_cons=1.225, λ_div=0.490, λ_align=0.012\n","Epoch 50/150 (2.9s) | Train Loss: 0.0943, Train Acc: 0.9649 | Test Acc: 0.9284, Test F1: 0.9279 | Best F1: 0.9469\n","  Mode Usage: M0:23.1% ▇▇ | M1:16.0% ▇ | M2:18.2% ▇ | M3:16.9% ▇ | M4: 8.4%  | M5:17.4% ▇\n","[Epoch 60] warmup=0.74, λ_cons=1.475, λ_div=0.590, λ_align=0.015\n","Epoch 60/150 (3.2s) | Train Loss: 0.0851, Train Acc: 0.9659 | Test Acc: 0.9410, Test F1: 0.9418 | Best F1: 0.9469\n","  Mode Usage: M0:18.6% ▇ | M1:13.8% ▇ | M2:18.4% ▇ | M3:19.3% ▇ | M4:13.6% ▇ | M5:16.3% ▇\n","[Epoch 70] warmup=0.86, λ_cons=1.725, λ_div=0.690, λ_align=0.017\n","Epoch 70/150 (2.6s) | Train Loss: 0.0832, Train Acc: 0.9701 | Test Acc: 0.9230, Test F1: 0.9241 | Best F1: 0.9469\n","  Mode Usage: M0:16.6% ▇ | M1:15.7% ▇ | M2:18.2% ▇ | M3:19.9% ▇ | M4:13.0% ▇ | M5:16.7% ▇\n","[Epoch 80] warmup=0.99, λ_cons=1.975, λ_div=0.790, λ_align=0.020\n","Epoch 80/150 (2.4s) | Train Loss: 0.0784, Train Acc: 0.9701 | Test Acc: 0.9308, Test F1: 0.9303 | Best F1: 0.9469\n","  Mode Usage: M0:16.1% ▇ | M1:16.2% ▇ | M2:18.2% ▇ | M3:17.6% ▇ | M4:15.1% ▇ | M5:16.7% ▇\n","[Epoch 90] warmup=1.00, λ_cons=2.000, λ_div=0.800, λ_align=0.020\n","Epoch 90/150 (2.7s) | Train Loss: 0.0705, Train Acc: 0.9732 | Test Acc: 0.9342, Test F1: 0.9349 | Best F1: 0.9469\n","  Mode Usage: M0:15.9% ▇ | M1:15.9% ▇ | M2:18.2% ▇ | M3:19.4% ▇ | M4:15.0% ▇ | M5:15.6% ▇\n","[Epoch 100] warmup=1.00, λ_cons=2.000, λ_div=0.800, λ_align=0.020\n","Epoch 100/150 (4.2s) | Train Loss: 0.0722, Train Acc: 0.9733 | Test Acc: 0.9369, Test F1: 0.9373 | Best F1: 0.9469\n","  Mode Usage: M0:15.2% ▇ | M1:16.8% ▇ | M2:17.7% ▇ | M3:20.0% ▇ | M4:16.0% ▇ | M5:14.4% ▇\n","[Epoch 110] warmup=1.00, λ_cons=2.000, λ_div=0.800, λ_align=0.020\n","Epoch 110/150 (2.6s) | Train Loss: 0.0646, Train Acc: 0.9769 | Test Acc: 0.9416, Test F1: 0.9414 | Best F1: 0.9469\n","  Mode Usage: M0:16.7% ▇ | M1:16.0% ▇ | M2:18.2% ▇ | M3:19.3% ▇ | M4:14.9% ▇ | M5:14.9% ▇\n","[Epoch 120] warmup=1.00, λ_cons=2.000, λ_div=0.800, λ_align=0.020\n","Epoch 120/150 (2.3s) | Train Loss: 0.0688, Train Acc: 0.9742 | Test Acc: 0.9379, Test F1: 0.9381 | Best F1: 0.9469\n","  Mode Usage: M0:15.7% ▇ | M1:16.2% ▇ | M2:17.9% ▇ | M3:19.4% ▇ | M4:15.5% ▇ | M5:15.3% ▇\n","[Epoch 130] warmup=1.00, λ_cons=2.000, λ_div=0.800, λ_align=0.020\n","Epoch 130/150 (2.3s) | Train Loss: 0.0598, Train Acc: 0.9780 | Test Acc: 0.9399, Test F1: 0.9402 | Best F1: 0.9469\n","  Mode Usage: M0:15.9% ▇ | M1:17.1% ▇ | M2:18.2% ▇ | M3:19.1% ▇ | M4:14.6% ▇ | M5:15.1% ▇\n","[Epoch 140] warmup=1.00, λ_cons=2.000, λ_div=0.800, λ_align=0.020\n","Epoch 140/150 (2.3s) | Train Loss: 0.0537, Train Acc: 0.9808 | Test Acc: 0.9318, Test F1: 0.9320 | Best F1: 0.9469\n","  Mode Usage: M0:16.1% ▇ | M1:16.3% ▇ | M2:18.2% ▇ | M3:19.6% ▇ | M4:15.2% ▇ | M5:14.5% ▇\n","[Epoch 150] warmup=1.00, λ_cons=2.000, λ_div=0.800, λ_align=0.020\n","Epoch 150/150 (2.4s) | Train Loss: 0.0618, Train Acc: 0.9774 | Test Acc: 0.9369, Test F1: 0.9370 | Best F1: 0.9469\n","  Mode Usage: M0:15.9% ▇ | M1:16.5% ▇ | M2:18.2% ▇ | M3:19.4% ▇ | M4:15.3% ▇ | M5:14.7% ▇\n","\n","================================================================================\n","Training completed!\n","Best Test F1-Score: 0.9469\n","Best Test Accuracy: 0.9467\n","================================================================================\n"]}],"source":["\"\"\"\n","Pure Switching Latent Dynamical Systems (SLDS) for UCI-HAR\n","Models activities as switching between multiple latent dynamics modes\n","WITHOUT Attractor-based State Flow components\n","\"\"\"\n","\n","import os\n","import time\n","import torch\n","import random\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n","\n","def set_seed(seed=42):\n","    \"\"\"\n","    랜덤 시드를 고정하여 실험의 재현성(Reproducibility)을 보장함\n","    \"\"\"\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)  # 멀티 GPU 사용 시\n","\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","\n","    # CuDNN 결정론적 모드 (속도는 약간 느려질 수 있지만 재현성 필수)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","\n","def seed_worker(worker_id):\n","    worker_seed = torch.initial_seed() % 2**32\n","    np.random.seed(worker_seed)\n","    random.seed(worker_seed)\n","\n","# ============================================================================\n","# UCI-HAR Dataset Loader\n","# ============================================================================\n","\n","class UCIHARDataset(Dataset):\n","    def __init__(self, data_path, split='train'):\n","        base_path = os.path.join(data_path, split, 'Inertial Signals')\n","\n","        signals = []\n","        signal_types = [\n","            'body_acc_x', 'body_acc_y', 'body_acc_z',\n","            'body_gyro_x', 'body_gyro_y', 'body_gyro_z',\n","            'total_acc_x', 'total_acc_y', 'total_acc_z'\n","        ]\n","\n","        for signal_type in signal_types:\n","            filename = f'{signal_type}_{split}.txt'\n","            filepath = os.path.join(base_path, filename)\n","            data = np.loadtxt(filepath)\n","            signals.append(data)\n","\n","        self.X = np.stack(signals, axis=-1)\n","\n","        label_path = os.path.join(data_path, split, f'y_{split}.txt')\n","        self.y = np.loadtxt(label_path).astype(np.int64) - 1\n","\n","        print(f'{split} set: {self.X.shape[0]} samples, {self.X.shape[1]} timesteps, {self.X.shape[2]} channels')\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        return torch.FloatTensor(self.X[idx]), torch.LongTensor([self.y[idx]])[0]\n","\n","\n","# ============================================================================\n","# Switching Dynamics Module\n","# ============================================================================\n","\n","class SwitchingDynamicsModule(nn.Module):\n","    \"\"\"\n","    Models latent dynamics as switching between M different modes\n","    s_{t+1} = F_{z_t}(s_t), where z_t ∈ {1, ..., M}\n","\n","    Each timestep has a mode assignment z_t learned from data\n","    \"\"\"\n","    def __init__(self, latent_dim, num_modes=6, hidden_dim=128):\n","        super().__init__()\n","        self.latent_dim = latent_dim\n","        self.num_modes = num_modes\n","\n","        # Mode-specific dynamics networks\n","        self.mode_dynamics = nn.ModuleList([\n","            nn.Sequential(\n","                nn.Linear(latent_dim, hidden_dim),\n","                nn.Tanh(),\n","                nn.Linear(hidden_dim, hidden_dim),\n","                nn.Tanh(),\n","                nn.Linear(hidden_dim, latent_dim),\n","                nn.Tanh()\n","            ) for _ in range(num_modes)\n","        ])\n","\n","        # Mode inference network (predicts mode from latent state)\n","        self.mode_predictor = nn.Sequential(\n","            nn.Linear(latent_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(hidden_dim, num_modes)\n","        )\n","\n","    def forward(self, s_sequence):\n","        \"\"\"\n","        Args:\n","            s_sequence: (B, T, D) sequence of latent states\n","        Returns:\n","            s_evolved: (B, T, D) evolved states\n","            mode_probs: (B, T, M) mode probabilities\n","            mode_assignments: (B, T) hard mode assignments\n","        \"\"\"\n","        batch_size, seq_len, _ = s_sequence.shape\n","\n","        # Predict mode probabilities for each timestep\n","        mode_logits = self.mode_predictor(s_sequence)  # (B, T, M)\n","        mode_probs = F.softmax(mode_logits, dim=-1)\n","        mode_assignments = mode_probs.argmax(dim=-1)  # (B, T)\n","\n","        # Apply mode-specific dynamics using soft assignment\n","        s_evolved = torch.zeros_like(s_sequence)\n","\n","        for m in range(self.num_modes):\n","            # Compute dynamics for mode m\n","            s_flat = s_sequence.reshape(-1, self.latent_dim)  # (B*T, D)\n","            delta_m = self.mode_dynamics[m](s_flat)  # (B*T, D)\n","            delta_m = delta_m.reshape(batch_size, seq_len, self.latent_dim)  # (B, T, D)\n","\n","            # Weight by mode probability\n","            mode_weight = mode_probs[:, :, m:m+1]  # (B, T, 1)\n","            s_evolved = s_evolved + mode_weight * (s_sequence + delta_m)\n","\n","        return s_evolved, mode_probs, mode_assignments\n","\n","    def compute_mode_consistency_loss(self, mode_probs):\n","        \"\"\"\n","        Encourage smooth mode transitions (adjacent timesteps similar modes)\n","        \"\"\"\n","        # (B, T, M) -> (B, T-1, M)\n","        mode_diff = mode_probs[:, 1:, :] - mode_probs[:, :-1, :]\n","        consistency_loss = (mode_diff ** 2).mean()\n","        return consistency_loss\n","\n","    def compute_mode_diversity_loss(self, mode_probs):\n","        \"\"\"\n","        Encourage using all available modes (avoid mode collapse)\n","\n","        기존: 엔트로피 최대화 (entropy) 기반\n","        변경: 모드 사용 빈도(mode_freq)가 균일 분포(1/M, ..., 1/M)에\n","             가깝도록 L2 (MSE) 거리로 유도\n","\n","        probs 기반 버전:\n","        - (B, T, M)를 전체 평균 내서 모드별 평균 사용률 p̄_m 계산\n","        - 이를 균일 분포(1/M, ..., 1/M)에 가깝게 만드는 MSE\n","        \"\"\"\n","        # mode_probs: (B, T, M)\n","        # 배치 + 타임스텝 전체에 대해 평균 사용률 (M,)\n","        mode_mean = mode_probs.mean(dim=(0, 1))  # (M,)\n","\n","        # 타겟: 완전히 균일한 분포 [1/M, 1/M, ..., 1/M]\n","        target = torch.full_like(mode_mean, 1.0 / self.num_modes)\n","\n","        # MSE 기반 loss (스케일 맞추기 위해 M 곱해줘도 되고, 안 곱해도 무방)\n","        diversity_loss = F.mse_loss(mode_mean, target) * self.num_modes\n","\n","        return diversity_loss\n","\n","\n","# ============================================================================\n","# Main Model\n","# ============================================================================\n","\n","class SLDS_HAR(nn.Module):\n","    \"\"\"\n","    Pure Switching Latent Dynamical Systems for HAR\n","    \"\"\"\n","    def __init__(self, input_dim=9, num_classes=6, num_modes=6, latent_dim=32, hidden_dim=64):\n","        super().__init__()\n","        self.num_modes = num_modes\n","        self.num_classes = num_classes\n","\n","        # Temporal encoder\n","        self.encoder = nn.Sequential(\n","            nn.Conv1d(input_dim, 64, kernel_size=5, padding=2),\n","            nn.BatchNorm1d(64),\n","            nn.ReLU(),\n","            nn.Conv1d(64, 128, kernel_size=5, padding=2),\n","            nn.BatchNorm1d(128),\n","            nn.ReLU(),\n","            nn.Conv1d(128, latent_dim, kernel_size=5, padding=2),\n","            nn.BatchNorm1d(latent_dim),\n","            nn.ReLU()\n","        )\n","\n","        # Switching dynamics\n","        self.switching_dynamics = SwitchingDynamicsModule(latent_dim, num_modes, hidden_dim)\n","\n","        # Temporal pooling\n","        self.temporal_pool = nn.AdaptiveAvgPool1d(1)\n","\n","        # Classifier\n","        self.classifier = nn.Sequential(\n","            nn.Linear(latent_dim, 64),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),  # 0.2\n","            nn.Linear(64, 32),\n","            nn.ReLU(),\n","            nn.Dropout(0.1),  # 0.1\n","            nn.Linear(32, num_classes)\n","        )\n","\n","        # Mode-to-class auxiliary classifier\n","        self.mode_class_predictor = nn.Linear(num_modes, num_classes)\n","\n","    def forward(self, x, return_switching_info=False):\n","        \"\"\"\n","        Args:\n","            x: (B, T, C) input time series\n","            return_switching_info: whether to return switching losses and info\n","        \"\"\"\n","        batch_size = x.size(0)\n","\n","        # Encode: (B, T, C) -> (B, C, T) -> (B, D, T)\n","        x = x.transpose(1, 2)\n","        latent_seq = self.encoder(x)  # (B, D, T)\n","        latent_seq = latent_seq.transpose(1, 2)  # (B, T, D)\n","\n","        # Apply switching dynamics\n","        latent_evolved, mode_probs, mode_assignments = self.switching_dynamics(latent_seq)\n","\n","        # Global average pooling\n","        latent_evolved = latent_evolved.transpose(1, 2)  # (B, D, T)\n","        latent_pooled = self.temporal_pool(latent_evolved).squeeze(-1)  # (B, D)\n","\n","        # Classification\n","        logits = self.classifier(latent_pooled)\n","\n","        # Switching information\n","        if return_switching_info:\n","            switching_info = {}\n","\n","            # Mode consistency loss\n","            consistency_loss = self.switching_dynamics.compute_mode_consistency_loss(mode_probs)\n","            switching_info['consistency'] = consistency_loss\n","\n","            # Mode diversity loss\n","            diversity_loss = self.switching_dynamics.compute_mode_diversity_loss(mode_probs)\n","            switching_info['diversity'] = diversity_loss\n","\n","            # Mode-class alignment\n","            mode_avg = mode_probs.mean(dim=1)  # (B, M)\n","            mode_class_logits = self.mode_class_predictor(mode_avg)\n","\n","            # mode_class_logits = mode_class_logits / 3.0\n","\n","            switching_info['mode_class_logits'] = mode_class_logits\n","\n","            return logits, switching_info, mode_assignments\n","        else:\n","            return logits, mode_assignments\n","\n","\n","# ============================================================================\n","# Training & Evaluation\n","# ============================================================================\n","\n","def train_epoch(model, train_loader, optimizer, device, lambda_cons, lambda_div, lambda_align=0.0):\n","    model.train()\n","    total_loss = 0\n","    all_preds, all_labels = [], []\n","\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        # Get predictions and switching information\n","        logits, switching_info, mode_assignments = model(data, return_switching_info=True)\n","\n","        # Classification loss\n","        ce_loss = F.cross_entropy(logits, target)\n","\n","        # Switching dynamics losses\n","        consistency_loss = switching_info['consistency']\n","        diversity_loss = switching_info['diversity']\n","\n","        # Mode-class alignment loss\n","        mode_class_logits = switching_info['mode_class_logits']\n","        alignment_loss = F.cross_entropy(mode_class_logits, target)\n","\n","        # Total loss\n","        loss = ce_loss + lambda_cons * consistency_loss + lambda_div * diversity_loss + lambda_align * alignment_loss\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        preds = logits.argmax(dim=1)\n","        all_preds.extend(preds.cpu().numpy())\n","        all_labels.extend(target.cpu().numpy())\n","\n","    acc = accuracy_score(all_labels, all_preds)\n","    return total_loss / len(train_loader), acc\n","\n","def evaluate(model, test_loader, device):\n","    model.eval()\n","    all_preds, all_labels = [], []\n","    all_mode_assignments = []\n","\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            logits, mode_assignments = model(data, return_switching_info=False)\n","            preds = logits.argmax(dim=1)\n","\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(target.cpu().numpy())\n","            all_mode_assignments.append(mode_assignments.cpu().numpy())\n","\n","    acc = accuracy_score(all_labels, all_preds)\n","    f1 = f1_score(all_labels, all_preds, average='macro')\n","    cm = confusion_matrix(all_labels, all_preds)\n","\n","    # Analyze mode usage\n","    all_mode_assignments = np.concatenate(all_mode_assignments, axis=0)  # (N, T)\n","    mode_usage = []\n","    for m in range(model.num_modes):\n","        usage = (all_mode_assignments == m).sum() / all_mode_assignments.size\n","        mode_usage.append(usage)\n","\n","    return acc, f1, cm, mode_usage\n","\n","\n","# ============================================================================\n","# Main Execution\n","# ============================================================================\n","\n","def main():\n","    set_seed(42)\n","    g = torch.Generator()\n","    g.manual_seed(42)\n","\n","    # Configuration\n","    data_path = '/content/drive/MyDrive/Colab Notebooks/HAR_data/UCI_HAR'\n","    batch_size = 64\n","    num_epochs = 150\n","    learning_rate = 0.001\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    # Loss weights\n","    lambda_cons = 1.0  # Mode consistency  # 1.0\n","    lambda_div = 0.5    # Mode diversity  # 0.5\n","    lambda_align = 0.1   # Mode-class alignment  # 0.1\n","\n","    print('=' * 80)\n","    print('Pure Switching Latent Dynamical Systems (SLDS)')\n","    print('=' * 80)\n","\n","    # Load datasets\n","    print('\\nLoading UCI-HAR dataset...')\n","    train_dataset = UCIHARDataset(data_path, split='train')\n","    test_dataset = UCIHARDataset(data_path, split='test')\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n","                              num_workers=2, worker_init_fn=seed_worker, generator=g)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n","                             num_workers=2, worker_init_fn=seed_worker, generator=g)\n","\n","    # Model\n","    print('\\nInitializing model...')\n","    model = SLDS_HAR(\n","        input_dim=9,\n","        num_classes=6,\n","        num_modes=6,  # Use 6 modes (same as num_classes)\n","        latent_dim=48,  # 48\n","        hidden_dim=128\n","    ).to(device)\n","\n","    print(f'Total parameters: {sum(p.numel() for p in model.parameters()):,}')\n","\n","    # Optimizer\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n","\n","    # Training loop\n","    print('\\nStarting training...')\n","    best_acc = 0\n","    best_f1 = 0\n","\n","    for epoch in range(num_epochs):\n","        start_time = time.time()\n","\n","        # =====================\n","        # 람다 Warm-Up\n","        # =====================\n","        warmup = min(1.0, epoch / 80.0)  # epoch 0~49: 0~0.98 / epoch 50 이후: 1.0 고정\n","\n","        lambda_cons_eff  = lambda_cons  * warmup\n","        lambda_div_eff   = lambda_div   * warmup\n","        lambda_align_eff = lambda_align * warmup\n","\n","        if (epoch + 1) % 10 == 0:\n","            print(f\"[Epoch {epoch+1}] warmup={warmup:.2f}, \"\n","                  f\"λ_cons={lambda_cons_eff:.3f}, λ_div={lambda_div_eff:.3f}, λ_align={lambda_align_eff:.3f}\")\n","\n","        train_loss, train_acc = train_epoch(model, train_loader, optimizer, device,\n","                                           lambda_cons_eff, lambda_div_eff, lambda_align_eff)  # lambda 수정\n","        test_acc, test_f1, cm, mode_usage = evaluate(model, test_loader, device)\n","\n","        scheduler.step()\n","\n","        epoch_time = time.time() - start_time\n","\n","        if test_f1 > best_f1:\n","            best_f1 = test_f1\n","            best_acc = test_acc # 기록용으로 같이 업데이트\n","            torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/best_slds_pure.pth')\n","\n","        if (epoch + 1) % 10 == 0:\n","            mode_str = ' | '.join([\n","                f'M{i}:{mode_usage[i]:5.1%} {\"▇\" * int(mode_usage[i] * 10)}'\n","                for i in range(len(mode_usage))\n","            ])\n","            print(f'Epoch {epoch+1}/{num_epochs} ({epoch_time:.1f}s) | '\n","                  f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | '\n","                  f'Test Acc: {test_acc:.4f}, Test F1: {test_f1:.4f} | '\n","                  f'Best F1: {best_f1:.4f}')\n","            print(f'  Mode Usage: {mode_str}')\n","\n","    print('\\n' + '=' * 80)\n","    print(f'Training completed!')\n","    print(f'Best Test F1-Score: {best_f1:.4f}')\n","    print(f'Best Test Accuracy: {best_acc:.4f}')\n","    print('=' * 80)\n","\n","\n","if __name__ == '__main__':\n","    main()"]}]}