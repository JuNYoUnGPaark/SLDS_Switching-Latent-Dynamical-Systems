{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNjyRT8DoMtD",
        "outputId": "7d08366f-9db9-4033-bf3c-d117a3aeadca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Pure Switching Latent Dynamical Systems (SLDS)\n",
            "================================================================================\n",
            "\n",
            "Loading UCI-HAR dataset...\n",
            "train set: 7352 samples, 128 timesteps, 9 channels\n",
            "test set: 2947 samples, 128 timesteps, 9 channels\n",
            "\n",
            "Initializing model...\n",
            "Total parameters: 198,054\n",
            "\n",
            "Starting training...\n",
            "[Epoch 10] warmup=0.11, λ_cons=0.113, λ_div=0.056, λ_align=0.001\n",
            "Epoch 10/150 (2.3s) | Train Loss: 0.1147, Train Acc: 0.9490 | Test Acc: 0.9389, Test F1: 0.9411 | Best F1: 0.9435\n",
            "  Mode Usage: M0:19.1% ▇ | M1: 9.9%  | M2:20.9% ▇▇ | M3:18.4% ▇ | M4:28.0% ▇▇ | M5: 3.7% \n",
            "[Epoch 20] warmup=0.24, λ_cons=0.237, λ_div=0.119, λ_align=0.002\n",
            "Epoch 20/150 (2.3s) | Train Loss: 0.1051, Train Acc: 0.9558 | Test Acc: 0.9291, Test F1: 0.9291 | Best F1: 0.9544\n",
            "  Mode Usage: M0:17.4% ▇ | M1:16.7% ▇ | M2:20.9% ▇▇ | M3:18.6% ▇ | M4:24.3% ▇▇ | M5: 2.1% \n",
            "[Epoch 30] warmup=0.36, λ_cons=0.362, λ_div=0.181, λ_align=0.004\n",
            "Epoch 30/150 (2.3s) | Train Loss: 0.1075, Train Acc: 0.9566 | Test Acc: 0.9427, Test F1: 0.9438 | Best F1: 0.9544\n",
            "  Mode Usage: M0:15.2% ▇ | M1:21.8% ▇▇ | M2:23.8% ▇▇ | M3:19.0% ▇ | M4:18.3% ▇ | M5: 1.9% \n",
            "[Epoch 40] warmup=0.49, λ_cons=0.487, λ_div=0.244, λ_align=0.005\n",
            "Epoch 40/150 (2.3s) | Train Loss: 0.0896, Train Acc: 0.9631 | Test Acc: 0.9460, Test F1: 0.9474 | Best F1: 0.9544\n",
            "  Mode Usage: M0:17.1% ▇ | M1:20.3% ▇▇ | M2:17.0% ▇ | M3:19.8% ▇ | M4:23.9% ▇▇ | M5: 1.8% \n",
            "[Epoch 50] warmup=0.61, λ_cons=0.613, λ_div=0.306, λ_align=0.006\n",
            "Epoch 50/150 (2.4s) | Train Loss: 0.0897, Train Acc: 0.9665 | Test Acc: 0.9338, Test F1: 0.9347 | Best F1: 0.9544\n",
            "  Mode Usage: M0:14.7% ▇ | M1: 7.1%  | M2:16.2% ▇ | M3:20.7% ▇▇ | M4:32.4% ▇▇▇ | M5: 8.8% \n",
            "[Epoch 60] warmup=0.74, λ_cons=0.738, λ_div=0.369, λ_align=0.007\n",
            "Epoch 60/150 (2.3s) | Train Loss: 0.0856, Train Acc: 0.9676 | Test Acc: 0.9386, Test F1: 0.9390 | Best F1: 0.9544\n",
            "  Mode Usage: M0:37.1% ▇▇▇ | M1:19.3% ▇ | M2:11.4% ▇ | M3: 2.9%  | M4:28.9% ▇▇ | M5: 0.3% \n",
            "[Epoch 70] warmup=0.86, λ_cons=0.863, λ_div=0.431, λ_align=0.009\n",
            "Epoch 70/150 (2.7s) | Train Loss: 0.0870, Train Acc: 0.9687 | Test Acc: 0.9413, Test F1: 0.9419 | Best F1: 0.9544\n",
            "  Mode Usage: M0:29.0% ▇▇ | M1:19.3% ▇ | M2: 8.9%  | M3:10.9% ▇ | M4:31.9% ▇▇▇ | M5: 0.1% \n",
            "[Epoch 80] warmup=0.99, λ_cons=0.988, λ_div=0.494, λ_align=0.010\n",
            "Epoch 80/150 (2.8s) | Train Loss: 0.0830, Train Acc: 0.9718 | Test Acc: 0.9457, Test F1: 0.9469 | Best F1: 0.9544\n",
            "  Mode Usage: M0:29.9% ▇▇ | M1:18.0% ▇ | M2: 1.4%  | M3:18.2% ▇ | M4:32.4% ▇▇▇ | M5: 0.0% \n",
            "[Epoch 90] warmup=1.00, λ_cons=1.000, λ_div=0.500, λ_align=0.010\n",
            "Epoch 90/150 (2.4s) | Train Loss: 0.0769, Train Acc: 0.9733 | Test Acc: 0.9464, Test F1: 0.9467 | Best F1: 0.9551\n",
            "  Mode Usage: M0:26.9% ▇▇ | M1:16.2% ▇ | M2:18.1% ▇ | M3: 1.1%  | M4:37.6% ▇▇▇ | M5: 0.0% \n",
            "[Epoch 100] warmup=1.00, λ_cons=1.000, λ_div=0.500, λ_align=0.010\n",
            "Epoch 100/150 (2.3s) | Train Loss: 0.0756, Train Acc: 0.9758 | Test Acc: 0.9511, Test F1: 0.9522 | Best F1: 0.9551\n",
            "  Mode Usage: M0:27.6% ▇▇ | M1: 8.6%  | M2:18.5% ▇ | M3: 2.0%  | M4:37.7% ▇▇▇ | M5: 5.5% \n",
            "[Epoch 110] warmup=1.00, λ_cons=1.000, λ_div=0.500, λ_align=0.010\n",
            "Epoch 110/150 (2.4s) | Train Loss: 0.0716, Train Acc: 0.9782 | Test Acc: 0.9518, Test F1: 0.9528 | Best F1: 0.9570\n",
            "  Mode Usage: M0:33.0% ▇▇▇ | M1: 5.3%  | M2:19.2% ▇ | M3: 0.0%  | M4:38.4% ▇▇▇ | M5: 4.2% \n",
            "[Epoch 120] warmup=1.00, λ_cons=1.000, λ_div=0.500, λ_align=0.010\n",
            "Epoch 120/150 (2.3s) | Train Loss: 0.0727, Train Acc: 0.9776 | Test Acc: 0.9474, Test F1: 0.9486 | Best F1: 0.9570\n",
            "  Mode Usage: M0:28.7% ▇▇ | M1: 4.8%  | M2:19.2% ▇ | M3: 4.0%  | M4:34.9% ▇▇▇ | M5: 8.3% \n",
            "[Epoch 130] warmup=1.00, λ_cons=1.000, λ_div=0.500, λ_align=0.010\n",
            "Epoch 130/150 (2.3s) | Train Loss: 0.0638, Train Acc: 0.9796 | Test Acc: 0.9515, Test F1: 0.9524 | Best F1: 0.9570\n",
            "  Mode Usage: M0:45.7% ▇▇▇▇ | M1: 6.1%  | M2:20.2% ▇▇ | M3: 1.2%  | M4:25.2% ▇▇ | M5: 1.6% \n",
            "[Epoch 140] warmup=1.00, λ_cons=1.000, λ_div=0.500, λ_align=0.010\n",
            "Epoch 140/150 (2.4s) | Train Loss: 0.0572, Train Acc: 0.9833 | Test Acc: 0.9464, Test F1: 0.9477 | Best F1: 0.9570\n",
            "  Mode Usage: M0:47.1% ▇▇▇▇ | M1: 5.8%  | M2:20.3% ▇▇ | M3: 0.6%  | M4:24.3% ▇▇ | M5: 1.9% \n",
            "[Epoch 150] warmup=1.00, λ_cons=1.000, λ_div=0.500, λ_align=0.010\n",
            "Epoch 150/150 (3.1s) | Train Loss: 0.0623, Train Acc: 0.9808 | Test Acc: 0.9467, Test F1: 0.9481 | Best F1: 0.9570\n",
            "  Mode Usage: M0:48.6% ▇▇▇▇ | M1: 5.7%  | M2:20.2% ▇▇ | M3: 0.6%  | M4:23.6% ▇▇ | M5: 1.3% \n",
            "\n",
            "================================================================================\n",
            "Training completed!\n",
            "Best Test F1-Score: 0.9570\n",
            "Best Test Accuracy: 0.9559\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Pure Switching Latent Dynamical Systems (SLDS) for UCI-HAR\n",
        "Models activities as switching between multiple latent dynamics modes\n",
        "WITHOUT Attractor-based State Flow components\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    \"\"\"\n",
        "    랜덤 시드를 고정하여 실험의 재현성(Reproducibility)을 보장함\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  # 멀티 GPU 사용 시\n",
        "\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "    # CuDNN 결정론적 모드 (속도는 약간 느려질 수 있지만 재현성 필수)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "# ============================================================================\n",
        "# UCI-HAR Dataset Loader\n",
        "# ============================================================================\n",
        "\n",
        "class UCIHARDataset(Dataset):\n",
        "    def __init__(self, data_path, split='train'):\n",
        "        base_path = os.path.join(data_path, split, 'Inertial Signals')\n",
        "\n",
        "        signals = []\n",
        "        signal_types = [\n",
        "            'body_acc_x', 'body_acc_y', 'body_acc_z',\n",
        "            'body_gyro_x', 'body_gyro_y', 'body_gyro_z',\n",
        "            'total_acc_x', 'total_acc_y', 'total_acc_z'\n",
        "        ]\n",
        "\n",
        "        for signal_type in signal_types:\n",
        "            filename = f'{signal_type}_{split}.txt'\n",
        "            filepath = os.path.join(base_path, filename)\n",
        "            data = np.loadtxt(filepath)\n",
        "            signals.append(data)\n",
        "\n",
        "        self.X = np.stack(signals, axis=-1)\n",
        "\n",
        "        label_path = os.path.join(data_path, split, f'y_{split}.txt')\n",
        "        self.y = np.loadtxt(label_path).astype(np.int64) - 1\n",
        "\n",
        "        print(f'{split} set: {self.X.shape[0]} samples, {self.X.shape[1]} timesteps, {self.X.shape[2]} channels')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.FloatTensor(self.X[idx]), torch.LongTensor([self.y[idx]])[0]\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Switching Dynamics Module\n",
        "# ============================================================================\n",
        "\n",
        "class SwitchingDynamicsModule(nn.Module):\n",
        "    \"\"\"\n",
        "    Models latent dynamics as switching between M different modes\n",
        "    s_{t+1} = F_{z_t}(s_t), where z_t ∈ {1, ..., M}\n",
        "\n",
        "    Each timestep has a mode assignment z_t learned from data\n",
        "    \"\"\"\n",
        "    def __init__(self, latent_dim, num_modes=6, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_modes = num_modes\n",
        "\n",
        "        # Mode-specific dynamics networks\n",
        "        self.mode_dynamics = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(latent_dim, hidden_dim),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(hidden_dim, latent_dim),\n",
        "                nn.Tanh()\n",
        "            ) for _ in range(num_modes)\n",
        "        ])\n",
        "\n",
        "        # Mode inference network (predicts mode from latent state)\n",
        "        self.mode_predictor = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, num_modes)\n",
        "        )\n",
        "\n",
        "    def forward(self, s_sequence):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            s_sequence: (B, T, D) sequence of latent states\n",
        "        Returns:\n",
        "            s_evolved: (B, T, D) evolved states\n",
        "            mode_probs: (B, T, M) mode probabilities\n",
        "            mode_assignments: (B, T) hard mode assignments\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = s_sequence.shape\n",
        "\n",
        "        # Predict mode probabilities for each timestep\n",
        "        mode_logits = self.mode_predictor(s_sequence)  # (B, T, M)\n",
        "        mode_probs = F.softmax(mode_logits, dim=-1)\n",
        "        mode_assignments = mode_probs.argmax(dim=-1)  # (B, T)\n",
        "\n",
        "        # Apply mode-specific dynamics using soft assignment\n",
        "        s_evolved = torch.zeros_like(s_sequence)\n",
        "\n",
        "        for m in range(self.num_modes):\n",
        "            # Compute dynamics for mode m\n",
        "            s_flat = s_sequence.reshape(-1, self.latent_dim)  # (B*T, D)\n",
        "            delta_m = self.mode_dynamics[m](s_flat)  # (B*T, D)\n",
        "            delta_m = delta_m.reshape(batch_size, seq_len, self.latent_dim)  # (B, T, D)\n",
        "\n",
        "            # Weight by mode probability\n",
        "            mode_weight = mode_probs[:, :, m:m+1]  # (B, T, 1)\n",
        "            s_evolved = s_evolved + mode_weight * (s_sequence + delta_m)\n",
        "\n",
        "        return s_evolved, mode_probs, mode_assignments\n",
        "\n",
        "    def compute_mode_consistency_loss(self, mode_probs):\n",
        "        \"\"\"\n",
        "        Encourage smooth mode transitions (adjacent timesteps similar modes)\n",
        "        \"\"\"\n",
        "        # (B, T, M) -> (B, T-1, M)\n",
        "        mode_diff = mode_probs[:, 1:, :] - mode_probs[:, :-1, :]\n",
        "        consistency_loss = (mode_diff ** 2).mean()\n",
        "        return consistency_loss\n",
        "\n",
        "    def compute_mode_diversity_loss(self, mode_probs):\n",
        "        \"\"\"\n",
        "        Encourage using all available modes (avoid mode collapse)\n",
        "\n",
        "        기존: 엔트로피 최대화 (entropy) 기반\n",
        "        변경: 모드 사용 빈도(mode_freq)가 균일 분포(1/M, ..., 1/M)에\n",
        "             가깝도록 L2 (MSE) 거리로 유도\n",
        "\n",
        "        probs 기반 버전:\n",
        "        - (B, T, M)를 전체 평균 내서 모드별 평균 사용률 p̄_m 계산\n",
        "        - 이를 균일 분포(1/M, ..., 1/M)에 가깝게 만드는 MSE\n",
        "        \"\"\"\n",
        "        # mode_probs: (B, T, M)\n",
        "        # 배치 + 타임스텝 전체에 대해 평균 사용률 (M,)\n",
        "        mode_mean = mode_probs.mean(dim=(0, 1))  # (M,)\n",
        "\n",
        "        # 타겟: 완전히 균일한 분포 [1/M, 1/M, ..., 1/M]\n",
        "        target = torch.full_like(mode_mean, 1.0 / self.num_modes)\n",
        "\n",
        "        # MSE 기반 loss (스케일 맞추기 위해 M 곱해줘도 되고, 안 곱해도 무방)\n",
        "        diversity_loss = F.mse_loss(mode_mean, target) * self.num_modes\n",
        "\n",
        "        return diversity_loss\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Main Model\n",
        "# ============================================================================\n",
        "\n",
        "class SLDS_HAR(nn.Module):\n",
        "    \"\"\"\n",
        "    Pure Switching Latent Dynamical Systems for HAR\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=9, num_classes=6, num_modes=6, latent_dim=32, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.num_modes = num_modes\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Temporal encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv1d(input_dim, 64, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(64, 128, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(128, latent_dim, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(latent_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Switching dynamics\n",
        "        self.switching_dynamics = SwitchingDynamicsModule(latent_dim, num_modes, hidden_dim)\n",
        "\n",
        "        # Temporal pooling\n",
        "        self.temporal_pool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),  # 0.2\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),  # 0.1\n",
        "            nn.Linear(32, num_classes)\n",
        "        )\n",
        "\n",
        "        # Mode-to-class auxiliary classifier\n",
        "        self.mode_class_predictor = nn.Linear(num_modes, num_classes)\n",
        "\n",
        "    def forward(self, x, return_switching_info=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (B, T, C) input time series\n",
        "            return_switching_info: whether to return switching losses and info\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Encode: (B, T, C) -> (B, C, T) -> (B, D, T)\n",
        "        x = x.transpose(1, 2)\n",
        "        latent_seq = self.encoder(x)  # (B, D, T)\n",
        "        latent_seq = latent_seq.transpose(1, 2)  # (B, T, D)\n",
        "\n",
        "        # Apply switching dynamics\n",
        "        latent_evolved, mode_probs, mode_assignments = self.switching_dynamics(latent_seq)\n",
        "\n",
        "        # Global average pooling\n",
        "        latent_evolved = latent_evolved.transpose(1, 2)  # (B, D, T)\n",
        "        latent_pooled = self.temporal_pool(latent_evolved).squeeze(-1)  # (B, D)\n",
        "\n",
        "        # Classification\n",
        "        logits = self.classifier(latent_pooled)\n",
        "\n",
        "        # Switching information\n",
        "        if return_switching_info:\n",
        "            switching_info = {}\n",
        "\n",
        "            # Mode consistency loss\n",
        "            consistency_loss = self.switching_dynamics.compute_mode_consistency_loss(mode_probs)\n",
        "            switching_info['consistency'] = consistency_loss\n",
        "\n",
        "            # Mode diversity loss\n",
        "            diversity_loss = self.switching_dynamics.compute_mode_diversity_loss(mode_probs)\n",
        "            switching_info['diversity'] = diversity_loss\n",
        "\n",
        "            # Mode-class alignment\n",
        "            mode_avg = mode_probs.mean(dim=1)  # (B, M)\n",
        "            mode_class_logits = self.mode_class_predictor(mode_avg)\n",
        "\n",
        "            mode_class_logits = mode_class_logits / 3.0\n",
        "\n",
        "            switching_info['mode_class_logits'] = mode_class_logits\n",
        "\n",
        "            return logits, switching_info, mode_assignments\n",
        "        else:\n",
        "            return logits, mode_assignments\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Training & Evaluation\n",
        "# ============================================================================\n",
        "\n",
        "def train_epoch(model, train_loader, optimizer, device, lambda_cons, lambda_div, lambda_align=0.0):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get predictions and switching information\n",
        "        logits, switching_info, mode_assignments = model(data, return_switching_info=True)\n",
        "\n",
        "        # Classification loss\n",
        "        ce_loss = F.cross_entropy(logits, target)\n",
        "\n",
        "        # Switching dynamics losses\n",
        "        consistency_loss = switching_info['consistency']\n",
        "        diversity_loss = switching_info['diversity']\n",
        "\n",
        "        # Mode-class alignment loss\n",
        "        mode_class_logits = switching_info['mode_class_logits']\n",
        "        alignment_loss = F.cross_entropy(mode_class_logits, target)\n",
        "\n",
        "        # Total loss\n",
        "        loss = ce_loss + lambda_cons * consistency_loss + lambda_div * diversity_loss + lambda_align * alignment_loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = logits.argmax(dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(target.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    return total_loss / len(train_loader), acc\n",
        "\n",
        "def evaluate(model, test_loader, device):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    all_mode_assignments = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            logits, mode_assignments = model(data, return_switching_info=False)\n",
        "            preds = logits.argmax(dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(target.cpu().numpy())\n",
        "            all_mode_assignments.append(mode_assignments.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # Analyze mode usage\n",
        "    all_mode_assignments = np.concatenate(all_mode_assignments, axis=0)  # (N, T)\n",
        "    mode_usage = []\n",
        "    for m in range(model.num_modes):\n",
        "        usage = (all_mode_assignments == m).sum() / all_mode_assignments.size\n",
        "        mode_usage.append(usage)\n",
        "\n",
        "    return acc, f1, cm, mode_usage\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Main Execution\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    set_seed(42)\n",
        "    g = torch.Generator()\n",
        "    g.manual_seed(42)\n",
        "\n",
        "    # Configuration\n",
        "    data_path = '/content/drive/MyDrive/Colab Notebooks/HAR_data/UCI_HAR'\n",
        "    batch_size = 64\n",
        "    num_epochs = 150\n",
        "    learning_rate = 0.001\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Loss weights\n",
        "    lambda_cons = 1.0  # Mode consistency  # 1.0\n",
        "    lambda_div = 0.5    # Mode diversity  # 0.5\n",
        "    lambda_align = 0.01   # Mode-class alignment  # 0.1\n",
        "\n",
        "    print('=' * 80)\n",
        "    print('Pure Switching Latent Dynamical Systems (SLDS)')\n",
        "    print('=' * 80)\n",
        "\n",
        "    # Load datasets\n",
        "    print('\\nLoading UCI-HAR dataset...')\n",
        "    train_dataset = UCIHARDataset(data_path, split='train')\n",
        "    test_dataset = UCIHARDataset(data_path, split='test')\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                              num_workers=2, worker_init_fn=seed_worker, generator=g)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
        "                             num_workers=2, worker_init_fn=seed_worker, generator=g)\n",
        "\n",
        "    # Model\n",
        "    print('\\nInitializing model...')\n",
        "    model = SLDS_HAR(\n",
        "        input_dim=9,\n",
        "        num_classes=6,\n",
        "        num_modes=6,  # Use 6 modes (same as num_classes)\n",
        "        latent_dim=48,  # 48\n",
        "        hidden_dim=96\n",
        "    ).to(device)\n",
        "\n",
        "    print(f'Total parameters: {sum(p.numel() for p in model.parameters()):,}')\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    # Training loop\n",
        "    print('\\nStarting training...')\n",
        "    best_acc = 0\n",
        "    best_f1 = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # =====================\n",
        "        # 람다 Warm-Up\n",
        "        # =====================\n",
        "        warmup = min(1.0, epoch / 80.0)  # epoch 0~49: 0~0.98 / epoch 50 이후: 1.0 고정\n",
        "\n",
        "        lambda_cons_eff  = lambda_cons  * warmup\n",
        "        lambda_div_eff   = lambda_div   * warmup\n",
        "        lambda_align_eff = lambda_align * warmup\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"[Epoch {epoch+1}] warmup={warmup:.2f}, \"\n",
        "                  f\"λ_cons={lambda_cons_eff:.3f}, λ_div={lambda_div_eff:.3f}, λ_align={lambda_align_eff:.3f}\")\n",
        "\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, device,\n",
        "                                           lambda_cons_eff, lambda_div_eff, lambda_align_eff)  # lambda 수정\n",
        "        test_acc, test_f1, cm, mode_usage = evaluate(model, test_loader, device)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "\n",
        "        if test_f1 > best_f1:\n",
        "            best_f1 = test_f1\n",
        "            best_acc = test_acc # 기록용으로 같이 업데이트\n",
        "            torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/best_slds_pure.pth')\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            mode_str = ' | '.join([\n",
        "                f'M{i}:{mode_usage[i]:5.1%} {\"▇\" * int(mode_usage[i] * 10)}'\n",
        "                for i in range(len(mode_usage))\n",
        "            ])\n",
        "            print(f'Epoch {epoch+1}/{num_epochs} ({epoch_time:.1f}s) | '\n",
        "                  f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | '\n",
        "                  f'Test Acc: {test_acc:.4f}, Test F1: {test_f1:.4f} | '\n",
        "                  f'Best F1: {best_f1:.4f}')\n",
        "            print(f'  Mode Usage: {mode_str}')\n",
        "\n",
        "    print('\\n' + '=' * 80)\n",
        "    print(f'Training completed!')\n",
        "    print(f'Best Test F1-Score: {best_f1:.4f}')\n",
        "    print(f'Best Test Accuracy: {best_acc:.4f}')\n",
        "    print('=' * 80)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ]
}